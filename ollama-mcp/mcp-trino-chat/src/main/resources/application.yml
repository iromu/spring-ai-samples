spring:
  application:
    name: mcp-trino-chat
  ai:
    ollama:
      chat:
        options:
          model: llama3.1:8b
  docker:
    compose:
      lifecycle-management: start_only

server:
  port: 7772

spring.ai.mcp.client.sse.connections.server1.url: http://localhost:7001
