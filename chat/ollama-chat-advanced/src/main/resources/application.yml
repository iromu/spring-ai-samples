spring:
  application:
    name: ollama-chat-advanced
  ai:
    ollama:
      chat:
        options:
          model: llama3.1:8b
server:
  port: 7771

spring.profiles.active: local

management.endpoint.health.probes.enabled: true
management.health.livenessState.enabled: true
management.health.readinessState.enabled: true

---
spring.config.activate.on-profile: docker
server.port: 80
