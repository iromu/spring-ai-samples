services:
  redis-stack-server:
    container_name: redis-stack-server
    image: redis/redis-stack-server:latest
    ports:
      - "9736:6379"

  mini-chat-ui:
    container_name: mini-chat-ui
    image: iromu/mini-chat-ui
    build:
      context: ./mini-chat-ui
    #      args:
    #        - VITE_BACKEND_URL=http://chat-backend:8080
    ports:
      - "3000:80"

# 1. This is SLOW
# First time boot needs to download 'llama3' model, 4.7 GB. If docker compose is started by Spring it will fail and stop the container.
# You have to manually start this container and wait for the model to download.
#
# 2. On EVERY application restart (with docker compose), ollama will load again the model into memory.
# This makes the rest endpoints "hang" until the next message shows on docker:
# msg="llama runner started in 149.95 seconds"

#  ollama:
#    image: ollama/ollama:latest
#    volumes:
#      - ./.ollama:/root/.ollama
#    tty: true
#    entrypoint: ["/usr/bin/bash", "/root/.ollama/ollama.sh"]
#    ports:
#      - 11434
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [ gpu ]
