services:
  redis-stack-server:
    container_name: redis-stack-server
    image: redis/redis-stack-server:latest
    ports:
      - "9736:6379"

  trino:
    container_name: trino
    image: trinodb/trino:latest
    ports:
      - "8384:8080"
    volumes:
      - ./docker/trino/etc:/etc/trino

  petstore:
    container_name: petstore
    image: swaggerapi/petstore:latest
    ports:
      - "8334:8080"
    environment:
      SWAGGER_HOST: http://localhost:8334
      SWAGGER_URL: http://localhost:8334
      SWAGGER_BASE_PATH: /v2


  # mini-chat-ui:
  #    container_name: mini-chat-ui
  #    image: iromu/mini-chat-ui:latest
  #    build:
  #      context: ./mini-chat-ui
  #    #      args:
  #    #        - VITE_BACKEND_URL=http://chat-backend:8080
  #    ports:
  #      - "3000:80"

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    ports:
      - "8000:8080"
    environment:
      WEBUI_AUTH: False
    extra_hosts:
      - "host.docker.internal:host-gateway"

volumes:
  open-webui:

# 1. This is SLOW
# First time boot needs to download 'llama3' model, 4.7 GB. If docker compose is started by Spring it will fail and stop the container.
# You have to manually start this container and wait for the model to download.
#
# 2. On EVERY application restart (with docker compose), ollama will load again the model into memory.
# This makes the rest endpoints "hang" until the next message shows on docker:
# msg="llama runner started in 149.95 seconds"

#  ollama:
#    image: ollama/ollama:latest
#    volumes:
#      - ./.ollama:/root/.ollama
#    tty: true
#    entrypoint: ["/usr/bin/bash", "/root/.ollama/ollama.sh"]
#    ports:
#      - 11434
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [ gpu ]
