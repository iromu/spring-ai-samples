services:
  redis-stack-server:
    container_name: redis-stack-server
    image: redis/redis-stack-server:latest
    ports:
      - "9736:6379"

  trino:
    container_name: trino
    image: trinodb/trino:latest
    ports:
      - "8384:8080"
    volumes:
      - ./docker/trino/etc:/etc/trino

  petstore:
    container_name: petstore
    image: openapitools/openapi-petstore
    ports:
      - "8334:8080"
    environment:
      SWAGGER_HOST: http://localhost:8334
      SWAGGER_URL: http://localhost:8334
      SWAGGER_BASE_PATH: /v3


  ai-gateway:
    container_name: ai-gateway
    image: iromu/ai-gateway:latest
    environment:
      VITE_OLLAMA: http://localhost:11888
    ports:
      - "3000:80"

  mini-chat-ui:
    container_name: mini-chat-ui
    image: iromu/mini-chat-ui:latest
    environment:
      VITE_OLLAMA: http://localhost:11888
    ports:
      - "3000:80"

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    ports:
      - "8000:8080"
    environment:
      WEBUI_AUTH: False
      OLLAMA_BASE_URL: http://host.docker.internal:11888
    extra_hosts:
      - "host.docker.internal:host-gateway"


# 1. This is SLOW
# First time boot needs to download 'llama3' model, 4.7 GB. If docker compose is started by Spring it will fail and stop the container.
# You have to manually start this container and wait for the model to download.
#
# 2. On EVERY application restart (with docker compose), ollama will load again the model into memory.
# This makes the rest endpoints "hang" until the next message shows on docker:
# msg="llama runner started in 149.95 seconds"

#  ollama:
#    image: ollama/ollama:latest
#    container_name: ollama
#    volumes:
#      - ./.ollama:/root/.ollama
#    tty: true
#    entrypoint: [ "/usr/bin/bash", "/root/.ollama/ollama.sh" ]
#    ports:
#      - 11434
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [ gpu ]


volumes:
  open-webui:
