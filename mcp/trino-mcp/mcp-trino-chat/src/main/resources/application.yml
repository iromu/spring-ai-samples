spring:
  application:
    name: mcp-trino-chat
  ai:
    ollama:
      chat:
        options:
          model: llama3.1:8b
    mcp:
      client:
        type: async
        sse:
          connections:
            mcp-server-trino:
              url: http://localhost:7001

server:
  port: 7774


logging:
  level:
    org.springframework.ai.chat.client.advisor: DEBUG

spring.profiles.active: local
